{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Required imports"
      ],
      "metadata": {
        "id": "UJmw8limybQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Va2SiQxx_hE",
        "outputId": "db5e38dd-882b-413c-89f2-6624fb4045ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JLbmGNBaxkJI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"./../\")\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from torch import einsum\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as dataf\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import io\n",
        "from scipy.io import loadmat as loadmat\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.nn.parameter import Parameter\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.nn import LayerNorm,Linear,Dropout,Softmax\n",
        "import time\n",
        "from PIL import Image\n",
        "import math\n",
        "from operator import truediv\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.backends.cudnn as cudnn\n",
        "import re\n",
        "from pathlib import Path\n",
        "import copy\n",
        "\n",
        "import utils\n",
        "import logger\n",
        "\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "A7arBzqz4Bct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HSI_LiDAR_DatasetTrain(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset='Trento'):\n",
        "\n",
        "        HSI = loadmat(f'./{dataset}11x11/HSI_Tr.mat')\n",
        "        LiDAR = loadmat(f'./{dataset}11x11/LIDAR_Tr.mat')\n",
        "        label = loadmat(f'./{dataset}11x11/TrLabel.mat')\n",
        "\n",
        "        self.hs_image = (torch.from_numpy(HSI['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
        "        self.lidar_image = (torch.from_numpy(LiDAR['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
        "        self.lbls = ((torch.from_numpy(label['Data'])-1).long()).reshape(-1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.hs_image.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.hs_image[i], self.lidar_image[i], self.lbls[i]\n",
        "\n",
        "class HSI_LiDAR_DatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset='Trento'):\n",
        "\n",
        "        HSI = loadmat(f'./{dataset}11x11/HSI_Te.mat')\n",
        "        LiDAR = loadmat(f'./{dataset}11x11/LIDAR_Te.mat')\n",
        "        label = loadmat(f'./{dataset}11x11/TeLabel.mat')\n",
        "\n",
        "        self.hs_image = (torch.from_numpy(HSI['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
        "        self.lidar_image = (torch.from_numpy(LiDAR['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
        "        self.lbls = ((torch.from_numpy(label['Data'])-1).long()).reshape(-1)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.hs_image.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.hs_image[i], self.lidar_image[i], self.lbls[i]"
      ],
      "metadata": {
        "id": "2ulJ3JAUyBZW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-HL Model"
      ],
      "metadata": {
        "id": "etdDdGaPyf61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HetConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,padding = None, bias = None,p = 64, g = 64):\n",
        "        super(HetConv, self).__init__()\n",
        "        # Groupwise Convolution\n",
        "        self.groupwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,groups=g,padding = kernel_size//3, stride = stride)\n",
        "        # Pointwise Convolution\n",
        "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1,groups=p, stride = stride)\n",
        "    def forward(self, x):\n",
        "        return self.groupwise_conv(x) + self.pointwise_conv(x)\n",
        "\n",
        "\n",
        "\n",
        "# Cross-HL Attention Module\n",
        "class CrossHL_attention(nn.Module):\n",
        "    def __init__(self, dim, patches, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.1, proj_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.dim = dim\n",
        "        self.Wq = nn.Linear(patches, dim * num_heads , bias=qkv_bias)\n",
        "        self.Wk = nn.Linear(dim, dim , bias=qkv_bias)\n",
        "        self.Wv = nn.Linear(patches+1, dim , bias=qkv_bias)\n",
        "        self.linear_projection = nn.Linear(dim * num_heads, dim)\n",
        "        self.linear_projection_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x, x2):\n",
        "\n",
        "        B, N, C = x.shape\n",
        "        # query vector using lidar data\n",
        "        query = self.Wq(x2).reshape(B, self.num_heads, self.num_heads, self.dim // self.num_heads).permute(0, 1, 2, 3)\n",
        "\n",
        "        key = self.Wk(x).reshape(B, N, self.num_heads, self.dim // self.num_heads).permute(0, 2, 1, 3)\n",
        "        value = self.Wv(x.transpose(1,2)).reshape(B, C, self.num_heads, self.dim // self.num_heads).permute(0, 2, 3, 1)\n",
        "        attention = torch.einsum('bhid,bhjd->bhij', key, query) * self.scale\n",
        "        attention = attention.softmax(dim=-1)\n",
        "\n",
        "        x = torch.einsum('bhij,bhjd->bhid', attention, value)\n",
        "        x = x.reshape(B, N, -1)\n",
        "        x = self.linear_projection(x)\n",
        "        x = self.linear_projection_drop(x)\n",
        "        return x\n",
        "\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "  def __init__(self, dim, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.fclayer1 = Linear(dim, mlp_dim)\n",
        "        self.fclayer2 = Linear(mlp_dim, dim)\n",
        "        self.act_fn = nn.GELU()\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "\n",
        "      nn.init.xavier_uniform_(self.fclayer1.weight)\n",
        "      nn.init.xavier_uniform_(self.fclayer2.weight)\n",
        "\n",
        "      nn.init.normal_(self.fclayer1.bias, std=1e-6)\n",
        "      nn.init.normal_(self.fclayer2.bias, std=1e-6)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.fclayer1(x)\n",
        "      x = self.act_fn(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fclayer2(x)\n",
        "      x = self.dropout(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "# Code for Single Encoder block contains both cross-hl attention and mlp layer\n",
        "class SingleEncoderBlock(nn.Module):\n",
        "    def __init__(self,dim, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.attention_norm = LayerNorm(dim, eps=1e-6) # First LayerNorm layer\n",
        "        self.ffn_norm = LayerNorm(dim, eps=1e-6) # Second LayerNorm layer\n",
        "\n",
        "        self.ffn = MultiLayerPerceptron(dim, mlp_dim) # MLP layer\n",
        "        self.cross_hl_attention = CrossHL_attention(dim = dim, patches = 11**2) # Cross-HL Attention layer\n",
        "    def forward(self, x1,x2):\n",
        "        res = x1\n",
        "        x = self.attention_norm(x1)\n",
        "        x= self.cross_hl_attention(x,x2)\n",
        "        x = x + res\n",
        "        res = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + res\n",
        "        return x\n",
        "\n",
        "# Code for Multiple Encoder blocks\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, mlp_dim=512, depth=2):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(dim, eps=1e-6)\n",
        "        for _ in range(depth):\n",
        "            layer = SingleEncoderBlock(dim, num_heads, mlp_dim)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, x,x2):\n",
        "        for layer_block in self.layer:\n",
        "            x = layer_block(x,x2)\n",
        "        encoded = self.encoder_norm(x)\n",
        "\n",
        "\n",
        "        return encoded[:,0] # Selecting only the first row of each batch since it is the cls token\n",
        "\n",
        "\n",
        "class CrossHL_Transformer(nn.Module):\n",
        "    def __init__(self, FM, NC, NCLidar, Classes,patchsize):\n",
        "        super(CrossHL_Transformer, self).__init__()\n",
        "        self.patchsize = patchsize\n",
        "        self.NCLidar = NCLidar\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv3d(1, 8, (9, 3, 3), padding=(0,1,1), stride = 1),\n",
        "            nn.BatchNorm3d(8),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.hetconv_layer = nn.Sequential(\n",
        "            HetConv(8 * (NC - 8), FM*4,\n",
        "                p = 1,\n",
        "                g = (FM*4)//4 if (8 * (NC - 8))%FM == 0 else (FM*4)//8,\n",
        "                   ),\n",
        "            nn.BatchNorm2d(FM*4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.ca = Encoder(FM*4)\n",
        "        self.fclayer = nn.Linear(FM*4 , Classes)\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, (patchsize**2) + 1, FM*4))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        torch.nn.init.xavier_uniform_(self.fclayer.weight)\n",
        "        torch.nn.init.normal_(self.fclayer.bias, std=1e-6)\n",
        "        # randomly initialized cls token\n",
        "        self.clsTok = nn.Parameter(torch.zeros(1, 1, FM*4))\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = x1.reshape(x1.shape[0],-1,self.patchsize,self.patchsize)\n",
        "        x2 = x2.reshape(x1.shape[0],-1,self.patchsize*self.patchsize)\n",
        "        x1 = x1.unsqueeze(1)\n",
        "        if(x2.shape[1] > 0):\n",
        "            x2 = F.adaptive_avg_pool1d(x2.flatten(2).transpose(1,2),1).transpose(1,2).reshape(x1.shape[0],-1,self.patchsize*self.patchsize)\n",
        "        x1 = self.conv5(x1)\n",
        "        x1 = x1.reshape(x1.shape[0],-1,self.patchsize,self.patchsize)\n",
        "\n",
        "        x1 = self.hetconv_layer(x1)\n",
        "        x1 = x1.flatten(2)\n",
        "\n",
        "        x1 = x1.transpose(-1, -2)\n",
        "        cls_tokens = self.clsTok.expand(x1.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x1), dim = 1)\n",
        "        x = x + self.position_embeddings\n",
        "        x = self.dropout(x)\n",
        "        x = self.ca(x,x2)\n",
        "        x = x.reshape(x.shape[0],-1)\n",
        "        out = self.fclayer(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5UcfGJSzyXsa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summary (for Trento Dataset)"
      ],
      "metadata": {
        "id": "88wE8ssv0fnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CrossHL_Transformer(FM=16, NC=63, NCLidar=1, Classes=6,patchsize = 11)\n",
        "summary(model, [(63, 11**2),(1,11**2)]) # summary(model, [(NC, patchsize**2),(NCLidar,patchsize**2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSJ8T6sa0muF",
        "outputId": "7379cf7c-be5d-486b-91b0-1540c891ab4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1        [-1, 8, 55, 11, 11]             656\n",
            "       BatchNorm3d-2        [-1, 8, 55, 11, 11]              16\n",
            "              ReLU-3        [-1, 8, 55, 11, 11]               0\n",
            "            Conv2d-4           [-1, 64, 11, 11]          31,744\n",
            "            Conv2d-5           [-1, 64, 11, 11]          28,224\n",
            "           HetConv-6           [-1, 64, 11, 11]               0\n",
            "       BatchNorm2d-7           [-1, 64, 11, 11]             128\n",
            "              ReLU-8           [-1, 64, 11, 11]               0\n",
            "           Dropout-9              [-1, 122, 64]               0\n",
            "        LayerNorm-10              [-1, 122, 64]             128\n",
            "           Linear-11               [-1, 1, 512]          61,952\n",
            "           Linear-12              [-1, 122, 64]           4,096\n",
            "           Linear-13               [-1, 64, 64]           7,808\n",
            "           Linear-14              [-1, 122, 64]          32,832\n",
            "          Dropout-15              [-1, 122, 64]               0\n",
            "CrossHL_attention-16              [-1, 122, 64]               0\n",
            "        LayerNorm-17              [-1, 122, 64]             128\n",
            "           Linear-18             [-1, 122, 512]          33,280\n",
            "             GELU-19             [-1, 122, 512]               0\n",
            "          Dropout-20             [-1, 122, 512]               0\n",
            "           Linear-21              [-1, 122, 64]          32,832\n",
            "          Dropout-22              [-1, 122, 64]               0\n",
            "MultiLayerPerceptron-23              [-1, 122, 64]               0\n",
            "SingleEncoderBlock-24              [-1, 122, 64]               0\n",
            "        LayerNorm-25              [-1, 122, 64]             128\n",
            "           Linear-26               [-1, 1, 512]          61,952\n",
            "           Linear-27              [-1, 122, 64]           4,096\n",
            "           Linear-28               [-1, 64, 64]           7,808\n",
            "           Linear-29              [-1, 122, 64]          32,832\n",
            "          Dropout-30              [-1, 122, 64]               0\n",
            "CrossHL_attention-31              [-1, 122, 64]               0\n",
            "        LayerNorm-32              [-1, 122, 64]             128\n",
            "           Linear-33             [-1, 122, 512]          33,280\n",
            "             GELU-34             [-1, 122, 512]               0\n",
            "          Dropout-35             [-1, 122, 512]               0\n",
            "           Linear-36              [-1, 122, 64]          32,832\n",
            "          Dropout-37              [-1, 122, 64]               0\n",
            "MultiLayerPerceptron-38              [-1, 122, 64]               0\n",
            "SingleEncoderBlock-39              [-1, 122, 64]               0\n",
            "        LayerNorm-40              [-1, 122, 64]             128\n",
            "          Encoder-41                   [-1, 64]               0\n",
            "           Linear-42                    [-1, 6]             390\n",
            "================================================================\n",
            "Total params: 407,398\n",
            "Trainable params: 407,398\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.52\n",
            "Forward/backward pass size (MB): 5.75\n",
            "Params size (MB): 1.55\n",
            "Estimated Total Size (MB): 10.83\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "-HEZeeiEzRL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "datasetNames = [\"Trento\"] # [\"Trento\", \"MUUFL\", \"Houston\"]\n",
        "MultiModalData = 'LiDAR'\n",
        "modelName = 'Cross-HL'\n",
        "\n",
        "patchsize = 11\n",
        "batch_size = 64 # batch size for training\n",
        "test_batch_size = 500\n",
        "EPOCHS = 10\n",
        "learning_rate = 5e-4\n",
        "FM = 16\n",
        "FileName = 'CrossHL'\n",
        "num_heads = 8 # d_h = number of mhsa heads\n",
        "mlp_dim = 512\n",
        "depth = 2 # Number of transformer encoder layer\n",
        "num_iterations = 3\n",
        "train_loss = []\n",
        "\n",
        "def seed_val(seed=14):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "for dataset in datasetNames:\n",
        "        print(f\"---------------------------------- Details for {dataset} dataset---------------------------------------------\")\n",
        "        print('\\n')\n",
        "        try:\n",
        "            os.makedirs(dataset)\n",
        "        except FileExistsError:\n",
        "            pass\n",
        "\n",
        "        train_dataset = HSI_LiDAR_DatasetTrain(dataset=dataset)\n",
        "        test_dataset = HSI_LiDAR_DatasetTest(dataset=dataset)\n",
        "        # print(len(train_dataset),len(test_dataset))\n",
        "        NC = train_dataset.hs_image.shape[1]\n",
        "\n",
        "        NCLidar = train_dataset.lidar_image.shape[1]\n",
        "        Classes = len(torch.unique(train_dataset.lbls))\n",
        "\n",
        "        train_loader = dataf.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers= 4)\n",
        "\n",
        "\n",
        "        test_patch_hsi = test_dataset.hs_image\n",
        "        test_patch_lidar = test_dataset.lidar_image\n",
        "        test_label = test_dataset.lbls\n",
        "\n",
        "        KAPPA = []\n",
        "        OA = []\n",
        "        AA = []\n",
        "        ELEMENT_ACC = np.zeros((num_iterations, Classes)) # num_iterationsxNC\n",
        "\n",
        "        seed_val(14)\n",
        "        for iterNum in range(num_iterations):\n",
        "            print('\\n')\n",
        "            print(\"---------------------------------- Summary ---------------------------------------------\")\n",
        "            print('\\n')\n",
        "            model = CrossHL_Transformer(FM=FM, NC=NC, NCLidar=NCLidar, Classes=Classes,patchsize = patchsize).cuda()\n",
        "            summary(model, [(NC, patchsize**2),(NCLidar,patchsize**2)])\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=5e-3)\n",
        "            loss_func = nn.CrossEntropyLoss()  # the target label is not one-hotted\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
        "            BestAcc = 0\n",
        "            torch.cuda.synchronize()\n",
        "            print('\\n')\n",
        "            print(f\"---------------------------------- Training started for {dataset} dataset ---------------------------------------------\")\n",
        "            print('\\n')\n",
        "            start = time.time()\n",
        "            # train and test the proposed model\n",
        "            for epoch in range(EPOCHS):\n",
        "                for step, (batch_hsi, batch_ldr, batch_lbl) in enumerate(train_loader):\n",
        "                    # print(f\"Step:{step} batch_hsi shape :{batch_hsi.shape} batch_ldr shape : {batch_ldr.shape} batch_lbl shape: {batch_lbl.shape}\")\n",
        "                    batch_hsi = batch_hsi.cuda()\n",
        "                    batch_ldr = batch_ldr.cuda()\n",
        "                    batch_lbl = batch_lbl.cuda()\n",
        "                    out= model(batch_hsi, batch_ldr)\n",
        "                    loss = loss_func(out, batch_lbl)\n",
        "                    optimizer.zero_grad()  # Clearing gradients\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if step % 50 == 0:\n",
        "                        model.eval()\n",
        "                        y_pred = np.empty((len(test_label)), dtype='float32')\n",
        "                        number = len(test_label) // test_batch_size\n",
        "                        for i in range(number):\n",
        "                            temp = test_patch_hsi[i * test_batch_size:(i + 1) * test_batch_size, :, :]\n",
        "                            temp = temp.cuda()\n",
        "                            temp1 = test_patch_lidar[i * test_batch_size:(i + 1) * test_batch_size, :, :]\n",
        "                            temp1 = temp1.cuda()\n",
        "                            temp2 = model(temp, temp1)\n",
        "                            temp3 = torch.max(temp2, 1)[1].squeeze()\n",
        "                            y_pred[i * test_batch_size:(i + 1) * test_batch_size] = temp3.cpu()\n",
        "                            del temp, temp1, temp2, temp3\n",
        "\n",
        "                        if (i + 1) * test_batch_size < len(test_label):\n",
        "                            temp = test_patch_hsi[(i + 1) * test_batch_size:len(test_label), :, :]\n",
        "                            temp = temp.cuda()\n",
        "                            temp1 = test_patch_lidar[(i + 1) * test_batch_size:len(test_label), :, :]\n",
        "                            temp1 = temp1.cuda()\n",
        "                            temp2 = model(temp, temp1)\n",
        "                            temp3 = torch.max(temp2, 1)[1].squeeze()\n",
        "                            y_pred[(i + 1) * test_batch_size:len(test_label)] = temp3.cpu()\n",
        "                            del temp, temp1, temp2, temp3\n",
        "\n",
        "                        y_pred = torch.from_numpy(y_pred).long()\n",
        "                        accuracy = torch.sum(y_pred == test_label).type(torch.FloatTensor) / test_label.size(0)\n",
        "\n",
        "                        print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.cpu().numpy(), '| test accuracy: %.4f' % (accuracy*100))\n",
        "                        train_loss.append(loss.data.cpu().numpy())\n",
        "\n",
        "                        if accuracy > BestAcc:\n",
        "\n",
        "                            BestAcc = accuracy\n",
        "\n",
        "                            torch.save(model.state_dict(), dataset+'/net_params_'+FileName+'.pkl')\n",
        "\n",
        "\n",
        "                        model.train()\n",
        "                scheduler.step()\n",
        "            torch.cuda.synchronize()\n",
        "            end = time.time()\n",
        "            print('\\nThe train time (in seconds) is:', end - start)\n",
        "            Train_time = end - start\n",
        "\n",
        "\n",
        "            model.load_state_dict(torch.load(dataset+'/net_params_'+FileName+'.pkl'))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            confusion_mat, overall_acc, class_acc, avg_acc, kappa_score = utils.result_reports(test_patch_hsi,test_patch_lidar,test_label,dataset,model, iterNum)\n",
        "            KAPPA.append(kappa_score)\n",
        "            OA.append(overall_acc)\n",
        "            AA.append(avg_acc)\n",
        "            ELEMENT_ACC[iterNum, :] = class_acc\n",
        "            torch.save(model, dataset+'/best_model_'+FileName+'_Iter'+str(iterNum)+'.pt')\n",
        "            print('\\n')\n",
        "            print(\"Overall Accuracy = \", overall_acc)\n",
        "            print('\\n')\n",
        "        print(f\"---------- Training Finished for {dataset} dataset -----------\")\n",
        "        print(\"\\nThe Confusion Matrix\")\n",
        "        logger.log_result(OA, AA, KAPPA, ELEMENT_ACC,'./' + dataset +'/'+FileName+'_Report_' + dataset +'.txt')"
      ],
      "metadata": {
        "id": "kw4nkJghzS_g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1JnqRahq2h_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}